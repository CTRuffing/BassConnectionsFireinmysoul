{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "cabf4843",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Read Hospital open/close dates from (Hospital_OpenCloseoverTime.xlsx) spreadsheet and create timelines for understanding what hospitals are open across what dates (call this availability timeline)\n",
        "\n",
        "\n",
        "#Read timelines of interest (string input) that we want to analyze attacks and catchment areas against hospitals for (call this data timeline)\n",
        "#Timelines of interest: Al Shifa Medical Hospital (Oct 7 2023, to Nov 3 2023, European Hospital: Dec 11 2023 to April 28 2024, Nasser Hospital: Nov 11 2024 to Feb 2 2025\n",
        "\n",
        "#Split data timeline into two week segments, if the last segment is less that 14 days it is okay (call this data aggregation method)\n",
        "\n",
        "#For each two week segment, compare data timeline (aka the two week time range selected) against availability timeline to determine when hospitals were open during data timeline\n",
        "\n",
        "#Determining catchment areas for hospitals that were open during data timeline: Based on the locations of the hospitals, find the catchment area based on the Voronoi method and geojson file. Catchment area of each hospital: Area where that hospital is the closest one and is within 5 km absolute distance. Determine relative to other hospitals open during timeline the areas that each hospital is the closest to, within abs dist of 5 km (call this catchment area method)\n",
        "\n",
        "#Weighted catchment area method: If the hospital list changes during data timeline segment, split segment into smaller segments where hospital list is constant. For each smaller segment, determine catchment areas for hospitals open during that smaller segment. Weight the catchment areas by the number of days each smaller segment represents in the overall data timeline segment. Combine weighted catchment areas to get overall weighted catchment area for data timeline segment (call this weighted catchment area method)\n",
        "\n",
        "#Attack count method: For each catchment area (only normal, not weighted), count the number of attacks that occurred within the catchment area of interest and during the data timeline segment of interest. This is based on the ACLED xlsx file.\n",
        "\n",
        "#Total Counts method: Within data timeline segment of interest, count total number of attacks that occurred during that time period (regardless of location)\n",
        "\n",
        "#Output method: For each data timeline segment of interest, output the following in an excel:\n",
        "#- Data timeline segment (start date, end date)\n",
        "#- For each hospital that was open during that data timeline segment:\n",
        "#  - Hospital name\n",
        "#  - Catchment area (km^2) (weighted if needed, but if no weighting needed just normal catchment area is also okay)\n",
        "#  - Number of attacks within catchment area during data timeline segment\n",
        "#  - Percentage of total attacks during data timeline segment that occurred within catchment area\n",
        "#  - Total Attacks during data timeline segment\n",
        "\n",
        "#Output Test HTML Map: For the first two week data timeline segment of Al Nasser, create an HTML map that shows:\n",
        "#- Hospital locations (markers with hospital names)\n",
        "#- Catchment areas (polygons with hospital that are open during that time period, based on Hospital open/close spreadsheet)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70bfa3fa",
      "metadata": {},
      "source": [
        "**Code Structure Summary**\n",
        "- **Imports & Config:** load required libraries and set file paths.\n",
        "- **load_hospitals(path):** read `Hospital_OpenCloseoverTime.xlsx`, parse toggle date columns (start at column D), build a GeoDataFrame with `open_intervals` per hospital.\n",
        "- **load_attacks(path):** read ACLED Excel (date at column B, lat/lon at W/X) and return GeoDataFrame with parsed dates.\n",
        "- **split_timeline(start,end,freq_days=14):** return list of (seg_start,seg_end) two-week windows.\n",
        "- **hospitals_open_in_range(hospitals_gdf,seg_start,seg_end):** select hospitals with >=1 day overlap based on `open_intervals`.\n",
        "- **subsegments_when_availability_changes(...):** split a segment into subsegments where the open-hospital set is constant and provide day-weights.\n",
        "- **compute_voronoi_catchments(hosp_gdf,boundary_gdf):** compute Voronoi polygons in a metric CRS, clip to boundary and return per-hospital polygons.\n",
        "- **limit_by_distance(polygons_gdf,hosp_gdf,max_km=5):** intersect polygons with 5 km geodesic buffers around each hospital.\n",
        "- **count_attacks_in_polygon(attacks_gdf,polygon,date_range):** count attacks inside polygon and within date range.\n",
        "- **export_results_excel(df,out_path):** write consolidated results to `02_10_Revised_Catchareas_Attackcounts.xlsx`.\n",
        "- **make_test_map(...):** produce an interactive `folium` HTML map for the Nasser segment (11/11/2024 â†’ +14 days).\n",
        "\n",
        "This notebook adds the above functions and a runner cell that orchestrates processing for the three timelines you provided."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f7788030",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and helper functions\n",
        "import os\n",
        "import pandas as pd\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point, Polygon, shape, mapping\n",
        "from shapely.ops import unary_union\n",
        "import numpy as np\n",
        "from datetime import timedelta\n",
        "import math\n",
        "from scipy.spatial import Voronoi\n",
        "import folium\n",
        "\n",
        "# File paths (adjust if needed)\n",
        "HOSPITALS_XLSX = 'Hospitals_OpenCloseoverTime.xlsx'\n",
        "ACLED_XLSX = 'ACLED_May_09_25_Gaza.xlsx'\n",
        "GEOJSON_BOUNDARY = 'gaza_boundary.geojson'\n",
        "OUTPUT_XLSX = '02_10_Revised_Catchareas_Attackcounts.xlsx'\n",
        "OUTPUT_HTML_NASSER = 'Nasser_2024-11-11_map.html'\n",
        "\n",
        "# Utility: parse dates robustly\n",
        "def parse_date(x):\n",
        "    if pd.isna(x):\n",
        "        return None\n",
        "    if hasattr(x, 'date'):\n",
        "        return pd.Timestamp(x)\n",
        "    for fmt in ('%m/%d/%Y', '%Y-%m-%d', '%d-%m-%Y'):\n",
        "        try:\n",
        "            return pd.to_datetime(str(x), format=fmt, errors='raise')\n",
        "        except Exception:\n",
        "            continue\n",
        "    return pd.to_datetime(x, errors='coerce')\n",
        "\n",
        "def _find_col(cols, opts):\n",
        "    for o in opts:\n",
        "        for c in cols:\n",
        "            if isinstance(c, str) and c.strip().lower().startswith(o.lower()):\n",
        "                return c\n",
        "    return None\n",
        "\n",
        "def load_hospitals(path):\n",
        "    \"\"\"Load hospital data with Open/Closed schedule. Uses column detection and schedule_meta from row 2.\"\"\"\n",
        "    raw_head = pd.read_excel(path, header=None, nrows=2)\n",
        "    full = pd.read_excel(path, header=0)\n",
        "    cols = list(full.columns)\n",
        "    hosp_col = _find_col(cols, ['hospital', 'name'])\n",
        "    lon_col = _find_col(cols, ['longitude (x)', 'longitude', 'lon', 'x'])\n",
        "    lat_col = _find_col(cols, ['latitude (y)', 'latitude', 'lat', 'y'])\n",
        "    if hosp_col is None or lon_col is None or lat_col is None:\n",
        "        raise ValueError(f\"Couldn't detect Hospital/lon/lat. Found: {cols}\")\n",
        "    # Detect Open/Closed columns with dates in row 2\n",
        "    schedule_meta = []\n",
        "    for c in cols:\n",
        "        if isinstance(c, str):\n",
        "            lc = c.strip().lower()\n",
        "            if lc.startswith('open') or lc.startswith('closed'):\n",
        "                typ = 'Open' if lc.startswith('open') else 'Closed'\n",
        "                try:\n",
        "                    idx = cols.index(c)\n",
        "                    raw = raw_head.iat[1, idx] if raw_head.shape[0] > 1 else None\n",
        "                    dt = pd.to_datetime(raw) if pd.notnull(raw) else None\n",
        "                except Exception:\n",
        "                    dt = None\n",
        "                if dt is not None:\n",
        "                    schedule_meta.append((c, typ, dt))\n",
        "    schedule_meta = [(c, t, d) for (c, t, d) in schedule_meta if d is not None]\n",
        "    # Fallback: per-row dates (Open/Closed columns contain dates in each cell)\n",
        "    open_cols = sorted([c for c in cols if isinstance(c, str) and 'open' in c.lower()], key=lambda x: (x.count('.'), x))\n",
        "    closed_cols = sorted([c for c in cols if isinstance(c, str) and 'closed' in c.lower()], key=lambda x: (x.count('.'), x))\n",
        "    records = []\n",
        "    for _, row in full.iterrows():\n",
        "        name = str(row[hosp_col]).strip()\n",
        "        lon = pd.to_numeric(row[lon_col], errors='coerce')\n",
        "        lat = pd.to_numeric(row[lat_col], errors='coerce')\n",
        "        if pd.isna(lon) or pd.isna(lat):\n",
        "            continue\n",
        "        open_intervals = []\n",
        "        if schedule_meta:\n",
        "            events = sorted([(pd.to_datetime(d).to_pydatetime(), col, typ) for (col, typ, d) in schedule_meta], key=lambda x: x[0])\n",
        "            changes = []\n",
        "            for dt, col, typ in events:\n",
        "                val = row.get(col, None) if col in row.index else None\n",
        "                if pd.notnull(val) and str(val).strip() != '':\n",
        "                    changes.append((dt, typ))\n",
        "            if not changes:\n",
        "                for dt, col, typ in events:\n",
        "                    changes.append((dt, typ))\n",
        "            changes = sorted(changes, key=lambda x: x[0])\n",
        "            compressed = []\n",
        "            for dt, typ in changes:\n",
        "                if not compressed or compressed[-1][1] != typ:\n",
        "                    compressed.append((dt, typ))\n",
        "            cur_start = None\n",
        "            for dt, typ in compressed:\n",
        "                if typ == 'Open':\n",
        "                    cur_start = pd.Timestamp(dt)\n",
        "                elif typ == 'Closed' and cur_start is not None:\n",
        "                    open_intervals.append((cur_start, pd.Timestamp(dt) - pd.Timedelta(days=1)))\n",
        "                    cur_start = None\n",
        "            if cur_start is not None:\n",
        "                open_intervals.append((cur_start, pd.Timestamp('2100-01-01')))\n",
        "        elif open_cols and closed_cols:\n",
        "            for oc, cc in zip(open_cols, closed_cols):\n",
        "                start = parse_date(row.get(oc))\n",
        "                end = parse_date(row.get(cc))\n",
        "                if start is not None and end is not None:\n",
        "                    open_intervals.append((pd.Timestamp(start), pd.Timestamp(end)))\n",
        "        if not open_intervals:\n",
        "            open_intervals = [(pd.Timestamp('1900-01-01'), pd.Timestamp('2100-01-01'))]\n",
        "        records.append({'name': name, 'longitude': float(lon), 'latitude': float(lat), 'open_intervals': open_intervals})\n",
        "    hg = gpd.GeoDataFrame(records, geometry=[Point(r['longitude'], r['latitude']) for r in records], crs='EPSG:4326')\n",
        "    return hg\n",
        "\n",
        "def _find_acled_col(df, opts):\n",
        "    \"\"\"Find first column matching any of opts (case-insensitive, allows spaces as underscores).\"\"\"\n",
        "    for col in df.columns:\n",
        "        norm = str(col).lower().replace(' ', '_').replace('-', '_')\n",
        "        for opt in opts:\n",
        "            if norm == opt or norm.startswith(opt + '_') or opt in norm:\n",
        "                return col\n",
        "    return None\n",
        "\n",
        "def load_attacks(path):\n",
        "    \"\"\"Load ACLED data with column name detection.\"\"\"\n",
        "    df = pd.read_excel(path, header=0)\n",
        "    date_col = _find_acled_col(df, ['event_date', 'date', 'iso_date', 'eventdate'])\n",
        "    lat_col = _find_acled_col(df, ['latitude', 'lat', 'y'])\n",
        "    lon_col = _find_acled_col(df, ['longitude', 'lon', 'long', 'x'])\n",
        "    if date_col is None or lat_col is None or lon_col is None:\n",
        "        raise ValueError(f\"Could not find date/lat/lon in ACLED. Columns: {list(df.columns)}\")\n",
        "    dates = pd.to_datetime(df[date_col], errors='coerce')\n",
        "    lats = pd.to_numeric(df[lat_col], errors='coerce')\n",
        "    lons = pd.to_numeric(df[lon_col], errors='coerce')\n",
        "    attacks = pd.DataFrame({'date': dates, 'latitude': lats, 'longitude': lons})\n",
        "    attacks = attacks.dropna(subset=['date', 'latitude', 'longitude']).copy()\n",
        "    attacks['date'] = pd.to_datetime(attacks['date']).dt.normalize()\n",
        "    ag = gpd.GeoDataFrame(attacks, geometry=[Point(xy) for xy in zip(attacks.longitude, attacks.latitude)], crs='EPSG:4326')\n",
        "    return ag\n",
        "\n",
        "def split_timeline(start, end, freq_days=14):\n",
        "    start = pd.to_datetime(start)\n",
        "    end = pd.to_datetime(end)\n",
        "    segments = []\n",
        "    cur = start\n",
        "    while cur <= end:\n",
        "            seg_end = min(end, cur + pd.Timedelta(days=freq_days-1))\n",
        "            segments.append((cur, seg_end))\n",
        "            cur = seg_end + pd.Timedelta(days=1)\n",
        "    return segments\n",
        "\n",
        "def intervals_overlap(a_start, a_end, b_start, b_end):\n",
        "    return (a_start <= b_end) and (b_start <= a_end)\n",
        "\n",
        "def hospitals_open_in_range(hosp_gdf, seg_start, seg_end):\n",
        "    seg_start = pd.to_datetime(seg_start).normalize()\n",
        "    seg_end = pd.to_datetime(seg_end).normalize()\n",
        "    rows = []\n",
        "    for idx, row in hosp_gdf.iterrows():\n",
        "        for (o_start, o_end) in row['open_intervals']:\n",
        "            o_start = pd.to_datetime(o_start).normalize()\n",
        "            o_end = pd.to_datetime(o_end).normalize()\n",
        "            if intervals_overlap(o_start, o_end, seg_start, seg_end):\n",
        "                # compute overlap days\n",
        "                os_ = max(o_start, seg_start)\n",
        "                oe_ = min(o_end, seg_end)\n",
        "                overlap_days = (oe_ - os_).days + 1\n",
        "                rows.append((idx, overlap_days))\n",
        "                break\n",
        "    if not rows:\n",
        "        return hosp_gdf.iloc[0:0]  # empty\n",
        "    idxs = [r[0] for r in rows]\n",
        "    subset = hosp_gdf.loc[idxs].copy()\n",
        "    return subset\n",
        "\n",
        "def voronoi_finite_polygons_2d(vor, radius=None):\n",
        "    # Credit: adapted from common recipes to construct finite polygons from scipy Voronoi\n",
        "    if vor.points.shape[1] != 2:\n",
        "        raise ValueError('Requires 2D input')\n",
        "    new_regions = []\n",
        "    new_vertices = vor.vertices.tolist()\n",
        "    center = vor.points.mean(axis=0)\n",
        "    if radius is None:\n",
        "        radius = np.ptp(vor.points, axis=0).max() * 2\n",
        "    all_ridges = {}\n",
        "    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
        "        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
        "        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
        "    for p1, region in enumerate(vor.point_region):\n",
        "        vertices = vor.regions[region]\n",
        "        if all(v >= 0 for v in vertices):\n",
        "            new_regions.append(vertices)\n",
        "            continue\n",
        "        ridges = all_ridges[p1]\n",
        "        new_region = [v for v in vertices if v>=0]\n",
        "        for p2, v1, v2 in ridges:\n",
        "            if v2 < 0 or v1 < 0:\n",
        "                v = v1 if v1>=0 else v2\n",
        "                tangent = vor.points[p2] - vor.points[p1]\n",
        "                tangent /= np.linalg.norm(tangent)\n",
        "                normal = np.array([-tangent[1], tangent[0]])\n",
        "                midpoint = vor.points[[p1,p2]].mean(axis=0)\n",
        "                direction = np.sign(np.dot(midpoint - center, normal)) * normal\n",
        "                far_point = vor.vertices[v] + direction * radius\n",
        "                new_vertices.append(far_point.tolist())\n",
        "                new_region.append(len(new_vertices)-1)\n",
        "        new_regions.append(new_region)\n",
        "    return new_regions, np.asarray(new_vertices)\n",
        "\n",
        "def compute_voronoi_catchments(hosp_gdf, boundary_gdf=None):\n",
        "    # Project to metric CRS for Voronoi (Web Mercator)\n",
        "    hosp_m = hosp_gdf.to_crs(epsg=3857).copy().reset_index(drop=True)\n",
        "    pts = np.array([[p.x, p.y] for p in hosp_m.geometry])\n",
        "    if len(pts) < 2:\n",
        "        # single hospital: entire clipped boundary (or buffer)\n",
        "        polys = []\n",
        "        for idx, row in hosp_m.iterrows():\n",
        "            if boundary_gdf is not None:\n",
        "                bnd = boundary_gdf if boundary_gdf.crs and boundary_gdf.crs.to_epsg() == 3857 else boundary_gdf.to_crs(epsg=3857)\n",
        "                poly = bnd.unary_union\n",
        "            else:\n",
        "                poly = row.geometry.buffer(50000)\n",
        "            polys.append(poly)\n",
        "        poly_gdf = gpd.GeoDataFrame({'geometry': polys, 'name': hosp_gdf['name'].values}, crs='EPSG:3857')\n",
        "        return poly_gdf, hosp_m\n",
        "    vor = Voronoi(pts)\n",
        "    regions, vertices = voronoi_finite_polygons_2d(vor)\n",
        "    polygons = []\n",
        "    for region in regions:\n",
        "        poly = Polygon(vertices[region])\n",
        "        polygons.append(poly)\n",
        "    poly_gdf = gpd.GeoDataFrame({'geometry': polygons}, crs='EPSG:3857')\n",
        "    # Clip to boundary if provided\n",
        "    if boundary_gdf is not None:\n",
        "        boundary_m = boundary_gdf.to_crs(epsg=3857)\n",
        "        clip_poly = boundary_m.unary_union\n",
        "        poly_gdf['geometry'] = poly_gdf.geometry.intersection(clip_poly)\n",
        "    # Assign back to hospitals order\n",
        "    poly_gdf = poly_gdf.reset_index(drop=True)\n",
        "    poly_gdf['name'] = hosp_gdf['name'].values\n",
        "    # Return in metric CRS (EPSG:3857) and original hospital points in same CRS for buffering steps\n",
        "    hosp_m = hosp_m.reset_index(drop=True)\n",
        "    return poly_gdf, hosp_m\n",
        "\n",
        "def limit_by_distance(polygons_gdf, hosp_gdf, max_km=5):\n",
        "    # polygons_gdf and hosp_gdf expected in same metric CRS (e.g., EPSG:3857)\n",
        "    max_m = max_km * 1000\n",
        "    clipped = []\n",
        "    for idx, row in polygons_gdf.iterrows():\n",
        "        hosp_point = hosp_gdf.loc[idx].geometry\n",
        "        buf = hosp_point.buffer(max_m)\n",
        "        inter = row.geometry.intersection(buf)\n",
        "        clipped.append(inter)\n",
        "    out = polygons_gdf.copy()\n",
        "    out['geometry'] = clipped\n",
        "    return out\n",
        "\n",
        "def count_attacks_in_polygon(attacks_gdf, poly, start_date, end_date):\n",
        "    if poly.is_empty:\n",
        "        return 0\n",
        "    attacks_clip = attacks_gdf[(attacks_gdf['date'] >= pd.to_datetime(start_date)) & (attacks_gdf['date'] <= pd.to_datetime(end_date))].copy()\n",
        "    if attacks_clip.empty:\n",
        "        return 0\n",
        "    attacks_clip = attacks_clip.to_crs(epsg=3857)\n",
        "    return int(attacks_clip.within(poly).sum())\n",
        "\n",
        "def export_results_excel(df, out_path):\n",
        "    df.to_excel(out_path, index=False)\n",
        "\n",
        "def make_test_map(hosp_gdf, catch_gdf, out_html, center=None):\n",
        "    # hosp_gdf and catch_gdf in EPSG:4326 for folium\n",
        "    if hosp_gdf.empty:\n",
        "        return\n",
        "    if center is None:\n",
        "        center = [float(hosp_gdf.geometry.y.mean()), float(hosp_gdf.geometry.x.mean())]\n",
        "    m = folium.Map(location=center, zoom_start=11)\n",
        "    for _, row in catch_gdf.iterrows():\n",
        "        geom = row.geometry\n",
        "        if geom is None or geom.is_empty:\n",
        "            continue\n",
        "        try:\n",
        "            geojson = mapping(geom)\n",
        "            folium.GeoJson(geojson, name=row.get('name', 'catchment')).add_to(m)\n",
        "        except Exception:\n",
        "            pass\n",
        "    for _, row in hosp_gdf.iterrows():\n",
        "        folium.Marker([float(row.geometry.y), float(row.geometry.x)], popup=str(row['name'])).add_to(m)\n",
        "    out_path = os.path.abspath(out_html)\n",
        "    m.save(out_path)\n",
        "\n",
        "# End of helper definitions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b6c70f38",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\qingl\\AppData\\Local\\Temp\\ipykernel_16472\\1408489956.py:234: DeprecationWarning: The 'unary_union' attribute is deprecated, use the 'union_all()' method instead.\n",
            "  clip_poly = boundary_m.unary_union\n"
          ]
        },
        {
          "ename": "GEOSException",
          "evalue": "TopologyException: side location conflict at 3810531.8687143112 3716467.8988773813. This can occur if the input geometry is invalid.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mGEOSException\u001b[39m                             Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     31\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     32\u001b[39m \u001b[38;5;66;03m# Compute Voronoi catchments for the open hospitals set\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m catch_v, hosp_m = \u001b[43mcompute_voronoi_catchments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopen_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboundary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Limit by 5 km around each hospital\u001b[39;00m\n\u001b[32m     35\u001b[39m catch_limited = limit_by_distance(catch_v, hosp_m, max_km=\u001b[32m5\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 235\u001b[39m, in \u001b[36mcompute_voronoi_catchments\u001b[39m\u001b[34m(hosp_gdf, boundary_gdf)\u001b[39m\n\u001b[32m    233\u001b[39m     boundary_m = boundary_gdf.to_crs(epsg=\u001b[32m3857\u001b[39m)\n\u001b[32m    234\u001b[39m     clip_poly = boundary_m.unary_union\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m     poly_gdf[\u001b[33m'\u001b[39m\u001b[33mgeometry\u001b[39m\u001b[33m'\u001b[39m] = \u001b[43mpoly_gdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgeometry\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintersection\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip_poly\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;66;03m# Assign back to hospitals order\u001b[39;00m\n\u001b[32m    237\u001b[39m poly_gdf = poly_gdf.reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qingl\\miniconda3\\Lib\\site-packages\\geopandas\\base.py:4856\u001b[39m, in \u001b[36mGeoPandasBase.intersection\u001b[39m\u001b[34m(self, other, align)\u001b[39m\n\u001b[32m   4745\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mintersection\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, align=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   4746\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a ``GeoSeries`` of the intersection of points in each\u001b[39;00m\n\u001b[32m   4747\u001b[39m \u001b[33;03m    aligned geometry with `other`.\u001b[39;00m\n\u001b[32m   4748\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   4854\u001b[39m \u001b[33;03m    GeoSeries.union\u001b[39;00m\n\u001b[32m   4855\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4856\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_binary_geo\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mintersection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qingl\\miniconda3\\Lib\\site-packages\\geopandas\\base.py:79\u001b[39m, in \u001b[36m_binary_geo\u001b[39m\u001b[34m(op, this, other, align, *args, **kwargs)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Binary operation on GeoSeries objects that returns a GeoSeries.\"\"\"\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mgeoseries\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GeoSeries\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m geoms, index = \u001b[43m_delegate_binary_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m GeoSeries(geoms, index=index, crs=this.crs)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qingl\\miniconda3\\Lib\\site-packages\\geopandas\\base.py:70\u001b[39m, in \u001b[36m_delegate_binary_method\u001b[39m\u001b[34m(op, this, other, align, *args, **kwargs)\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m(this), \u001b[38;5;28mtype\u001b[39m(other))\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m data = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ma_this\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data, this.index\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qingl\\miniconda3\\Lib\\site-packages\\geopandas\\array.py:842\u001b[39m, in \u001b[36mGeometryArray.intersection\u001b[39m\u001b[34m(self, other)\u001b[39m\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mintersection\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[32m    841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m GeometryArray(\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_binary_method\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mintersection\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m, crs=\u001b[38;5;28mself\u001b[39m.crs\n\u001b[32m    843\u001b[39m     )\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qingl\\miniconda3\\Lib\\site-packages\\geopandas\\array.py:779\u001b[39m, in \u001b[36mGeometryArray._binary_method\u001b[39m\u001b[34m(op, left, right, **kwargs)\u001b[39m\n\u001b[32m    776\u001b[39m         _crs_mismatch_warn(left, right, stacklevel=\u001b[32m7\u001b[39m)\n\u001b[32m    777\u001b[39m     right = right._data\n\u001b[32m--> \u001b[39m\u001b[32m779\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshapely\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qingl\\miniconda3\\Lib\\site-packages\\shapely\\decorators.py:173\u001b[39m, in \u001b[36mdeprecate_positional.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m173\u001b[39m     result = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     n = \u001b[38;5;28mlen\u001b[39m(args)\n\u001b[32m    176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m n > warn_from:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qingl\\miniconda3\\Lib\\site-packages\\shapely\\decorators.py:88\u001b[39m, in \u001b[36mmultithreading_enabled.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m array_args:\n\u001b[32m     87\u001b[39m         arr.flags.writeable = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m arr, old_flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(array_args, old_flags):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\qingl\\miniconda3\\Lib\\site-packages\\shapely\\set_operations.py:168\u001b[39m, in \u001b[36mintersection\u001b[39m\u001b[34m(a, b, grid_size, **kwargs)\u001b[39m\n\u001b[32m    164\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgrid_size parameter only accepts scalar values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    166\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m lib.intersection_prec(a, b, grid_size, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mintersection\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mGEOSException\u001b[39m: TopologyException: side location conflict at 3810531.8687143112 3716467.8988773813. This can occur if the input geometry is invalid."
          ]
        }
      ],
      "source": [
        "# Runner: orchestrate using the timelines you provided\n",
        "# Timelines of interest (user-provided):\n",
        "timelines = {\n",
        "    'Al Shifa Medical Hospital': ('2023-10-07','2023-11-03'),\n",
        "    'European Hospital': ('2023-12-11','2024-04-28'),\n",
        "    'Nasser Hospital': ('2024-11-11','2025-02-02')\n",
        "}\n",
        "# Load data\n",
        "hosp = load_hospitals(HOSPITALS_XLSX)\n",
        "attacks = load_attacks(ACLED_XLSX)\n",
        "boundary = None\n",
        "try:\n",
        "    if os.path.exists(GEOJSON_BOUNDARY):\n",
        "        boundary = gpd.read_file(GEOJSON_BOUNDARY)\n",
        "        if boundary.crs is None:\n",
        "            boundary = boundary.set_crs('EPSG:4326')\n",
        "    else:\n",
        "        boundary = None\n",
        "except Exception as e:\n",
        "    print(f\"Could not load boundary: {e}\")\n",
        "    boundary = None\n",
        "\n",
        "results = []\n",
        "for hosp_name, (tstart, tend) in timelines.items():\n",
        "    segments = split_timeline(tstart, tend, freq_days=14)\n",
        "    for seg_start, seg_end in segments:\n",
        "        open_h = hospitals_open_in_range(hosp, seg_start, seg_end)\n",
        "        if open_h.empty:\n",
        "            # record zero results for this segment\n",
        "            results.append({'hospital_of_interest': hosp_name, 'segment_start': seg_start, 'segment_end': seg_end, 'hospital_name': None, 'catchment_km2': 0.0, 'attacks_in_catchment': 0, 'pct_of_total': 0.0, 'total_attacks_segment': 0})\n",
        "            continue\n",
        "        # Compute Voronoi catchments for the open hospitals set\n",
        "        catch_v, hosp_m = compute_voronoi_catchments(open_h, boundary)\n",
        "        # Limit by 5 km around each hospital\n",
        "        catch_limited = limit_by_distance(catch_v, hosp_m, max_km=5)\n",
        "        # Convert catchment back to 4326 for area calculation and attack counting (we'll compute area in metric CRS)\n",
        "        catch4326 = catch_limited.to_crs(epsg=4326)\n",
        "        # total attacks in this segment (anywhere)\n",
        "        total_attacks = int(((attacks['date'] >= pd.to_datetime(seg_start)) & (attacks['date'] <= pd.to_datetime(seg_end))).sum())\n",
        "        # For each hospital polygon, compute area and count attacks (non-weighted catchments as requested)\n",
        "        for idx, row in catch_limited.reset_index(drop=True).iterrows():\n",
        "            poly_metric = row.geometry\n",
        "            if poly_metric is None or poly_metric.is_empty:\n",
        "                area_km2 = 0.0\n",
        "            else:\n",
        "                area_km2 = poly_metric.area / 1e6\n",
        "            # Count attacks (convert polygon to 4326 for spatial test)\n",
        "            poly_4326 = gpd.GeoSeries([poly_metric], crs='EPSG:3857').to_crs(epsg=4326).iat[0]\n",
        "            attacks_in = 0\n",
        "            if not poly_4326.is_empty:\n",
        "                attacks_in = count_attacks_in_polygon(attacks, poly_metric, seg_start, seg_end)\n",
        "            pct = (attacks_in / total_attacks * 100.0) if total_attacks>0 else 0.0\n",
        "            results.append({'hospital_of_interest': hosp_name, 'segment_start': seg_start, 'segment_end': seg_end, 'hospital_name': row.get('name'), 'catchment_km2': area_km2, 'attacks_in_catchment': attacks_in, 'pct_of_total': pct, 'total_attacks_segment': total_attacks})\n",
        "\n",
        "# Export results as requested: single sheet for all three hospitals\n",
        "resdf = pd.DataFrame(results)\n",
        "out_xlsx_path = os.path.abspath(OUTPUT_XLSX)\n",
        "export_results_excel(resdf, out_xlsx_path)\n",
        "\n",
        "# Create HTML map for Nasser first two-week segment (11/11/2024 + 14 days)\n",
        "nasser_start = pd.to_datetime('2024-11-11')\n",
        "nasser_end = nasser_start + pd.Timedelta(days=13)\n",
        "open_nasser = hospitals_open_in_range(hosp, nasser_start, nasser_end)\n",
        "if not open_nasser.empty:\n",
        "    catch_v, hosp_m = compute_voronoi_catchments(open_nasser, boundary)\n",
        "    catch_limited = limit_by_distance(catch_v, hosp_m, max_km=5)\n",
        "    hosp4326 = open_nasser.copy()\n",
        "    hosp4326 = hosp4326.to_crs(epsg=4326)\n",
        "    catch4326 = catch_limited.to_crs(epsg=4326)\n",
        "    if 'name' not in catch4326.columns:\n",
        "        catch4326['name'] = hosp4326['name'].values\n",
        "    make_test_map(hosp4326, catch4326, OUTPUT_HTML_NASSER)\n",
        "    print(f'Nasser map saved to: {os.path.abspath(OUTPUT_HTML_NASSER)}')\n",
        "else:\n",
        "    print(f'No hospitals open during Nasser segment {nasser_start.date()} to {nasser_end.date()}; skipping HTML map.')\n",
        "\n",
        "print('Runner finished. Results written to', out_xlsx_path)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
