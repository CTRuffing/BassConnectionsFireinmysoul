{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c902279",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading files...\n",
      "Using date column: event_date, lat/lon: latitude/longitude, sub-event column: sub_event_type\n",
      "Hospital lat/lon columns: Latitude (Y)/Longitude (X)\n",
      "\n",
      "============================================================\n",
      "Processing date range #1: 2023-10-07 to 2023-11-03\n",
      "Found 1107 events in this period.\n",
      "\n",
      "============================================================\n",
      "Processing date range #2: 2023-12-11 to 2024-04-28\n",
      "Found 5517 events in this period.\n",
      "\n",
      "============================================================\n",
      "Processing date range #3: 2024-11-11 to 2025-02-02\n",
      "Found 2163 events in this period.\n",
      "\n",
      "Saved combined binned counts to: .\\injuries.xlsx\n",
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ctr22\\AppData\\Local\\Temp\\ipykernel_8280\\77958883.py:201: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(['time_bin', 'attack_category'])\n",
      "C:\\Users\\ctr22\\AppData\\Local\\Temp\\ipykernel_8280\\77958883.py:201: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(['time_bin', 'attack_category'])\n",
      "C:\\Users\\ctr22\\AppData\\Local\\Temp\\ipykernel_8280\\77958883.py:201: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  .groupby(['time_bin', 'attack_category'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "\n",
    "# --------- User inputs (edit if needed) ----------\n",
    "acled_path = \"ACLED_May_09_25_Gaza.xlsx\"\n",
    "hosp_path  = \"Hospitals_OpenCloseoverTime.xlsx\"\n",
    "\n",
    "date_ranges = [\n",
    "    (datetime(2023, 10, 7), datetime(2023, 11, 3)),\n",
    "    (datetime(2023, 12, 11), datetime(2024, 4, 28)),\n",
    "    (datetime(2024, 11, 11), datetime(2025, 2, 2)),\n",
    "]\n",
    "\n",
    "# This dictionary maps the final wide-column labels (keys) to the list of ACLED sub-event values that\n",
    "# should count toward that column. Adjust as needed.\n",
    "attack_categories = {\n",
    "    'Air/drone strike': ['Air/drone strike'],\n",
    "    'Ground Attacks': ['Armed clash', 'Attack', 'Remote explosive/landmine/IED', 'Grenade', 'Change to group/activity'],\n",
    "    'Shelling/artillery/missile attack': ['Shelling/artillery/missile attack'],\n",
    "    'Civil Unrest': ['Mob violence', 'Looting/property destruction', 'Violent demonstration', 'Peaceful protest',\n",
    "                     'Disrupted weapons use', 'Excessive force against protesters', 'Arrests',\n",
    "                     'Protest with intervention', 'Abduction/forced disappearance', 'Sexual violence'],\n",
    "    'Other': ['Other']\n",
    "}\n",
    "\n",
    "# adjustable bin length (2 weeks by default)\n",
    "BIN_LENGTH = timedelta(days=14)\n",
    "\n",
    "# column detection candidates (edit if you want to hardcode names)\n",
    "ACLED_DATE_COLS = ['event_date', 'date', 'Event Date', 'event_date_ymd']\n",
    "ACLED_LAT_COLS = ['latitude', 'Latitude', 'LAT']\n",
    "ACLED_LON_COLS = ['longitude', 'Longitude', 'LON']\n",
    "SUB_EVENT_COLS = ['sub_event_type', 'Sub Event Type', 'sub-event_type', 'sub_event', 'sub_eventtype']\n",
    "\n",
    "HOSP_LAT_COLS = ['Latitude (Y)', 'Latitude', 'latitude', 'LAT']\n",
    "HOSP_LON_COLS = ['Longitude (X)', 'Longitude', 'longitude', 'LON']\n",
    "\n",
    "output_folder = \".\"  # change if you'd like a different output location\n",
    "# -------------------------------------------------\n",
    "\n",
    "def find_col(df, candidates):\n",
    "    cols_lower = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand in df.columns:\n",
    "            return cand\n",
    "        key = cand.lower()\n",
    "        if key in cols_lower:\n",
    "            return cols_lower[key]\n",
    "    for c in df.columns:\n",
    "        if c.replace(\" \", \"\").lower() in [x.replace(\" \", \"\").lower() for x in candidates]:\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def haversine_np(lon1, lat1, lon2, lat2):\n",
    "    # all inputs are numpy arrays (can be 1D or 2D), function returns distances in km\n",
    "    lon1, lat1, lon2, lat2 = map(np.radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1\n",
    "    dlat = lat2 - lat1\n",
    "    a = np.sin(dlat/2.0)**2 + np.cos(lat1) * np.cos(lat2) * np.sin(dlon/2.0)**2\n",
    "    c = 2 * np.arcsin(np.sqrt(a))\n",
    "    km = 6371.0 * c\n",
    "    return km\n",
    "\n",
    "def map_sub_to_category_map(attack_categories):\n",
    "    m = {}\n",
    "    for cat, subs in attack_categories.items():\n",
    "        for s in subs:\n",
    "            m[s.strip().lower()] = cat\n",
    "    return m\n",
    "\n",
    "def build_bin_edges(start_dt, end_dt, bin_length):\n",
    "    edges = []\n",
    "    cur = pd.Timestamp(start_dt)\n",
    "    # create edges inclusive of start; we'll add final edge after loop to make end inclusive\n",
    "    while cur <= pd.Timestamp(end_dt):\n",
    "        edges.append(cur)\n",
    "        cur = cur + bin_length\n",
    "    # final edge is end_dt + 1 day so pd.cut with right=False will include events on end_dt\n",
    "    edges.append(pd.Timestamp(end_dt) + pd.Timedelta(days=1))\n",
    "    return edges\n",
    "\n",
    "def ensure_all_categories(df, categories):\n",
    "    \"\"\"Ensure df has columns for every category in categories; fill missing with zeros.\"\"\"\n",
    "    for c in categories:\n",
    "        if c not in df.columns:\n",
    "            df[c] = 0\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    print(\"Loading files...\")\n",
    "    acled = pd.read_excel(acled_path, engine='openpyxl')\n",
    "    hospitals = pd.read_excel(hosp_path, engine='openpyxl')\n",
    "\n",
    "    date_col = find_col(acled, ACLED_DATE_COLS)\n",
    "    lat_col = find_col(acled, ACLED_LAT_COLS)\n",
    "    lon_col = find_col(acled, ACLED_LON_COLS)\n",
    "    sub_event_col = find_col(acled, SUB_EVENT_COLS)\n",
    "\n",
    "    hosp_lat = find_col(hospitals, HOSP_LAT_COLS)\n",
    "    hosp_lon = find_col(hospitals, HOSP_LON_COLS)\n",
    "\n",
    "    if date_col is None:\n",
    "        raise ValueError(\"Could not find a date column in ACLED. Candidates tried: \" + str(ACLED_DATE_COLS))\n",
    "    if lat_col is None or lon_col is None:\n",
    "        raise ValueError(\"Could not find latitude/longitude columns in ACLED. Candidates tried: \" + str(ACLED_LAT_COLS) + \", \" + str(ACLED_LON_COLS))\n",
    "    if hosp_lat is None or hosp_lon is None:\n",
    "        raise ValueError(\"Could not find Latitude/Longitude in hospitals file. Candidates tried: \" + str(HOSP_LAT_COLS) + \", \" + str(HOSP_LON_COLS))\n",
    "    if sub_event_col is None:\n",
    "        if 'sub_event_type' in acled.columns:\n",
    "            sub_event_col = 'sub_event_type'\n",
    "        else:\n",
    "            raise ValueError(\"Sub-event column not found. Ensure there is a sub-event column (e.g., 'sub_event_type').\")\n",
    "\n",
    "    print(f\"Using date column: {date_col}, lat/lon: {lat_col}/{lon_col}, sub-event column: {sub_event_col}\")\n",
    "    print(f\"Hospital lat/lon columns: {hosp_lat}/{hosp_lon}\")\n",
    "\n",
    "    # Standardize types\n",
    "    acled[date_col] = pd.to_datetime(acled[date_col], errors='coerce')\n",
    "    acled = acled.dropna(subset=[date_col])\n",
    "    acled[lat_col] = pd.to_numeric(acled[lat_col], errors='coerce')\n",
    "    acled[lon_col] = pd.to_numeric(acled[lon_col], errors='coerce')\n",
    "    hospitals[hosp_lat] = pd.to_numeric(hospitals[hosp_lat], errors='coerce')\n",
    "    hospitals[hosp_lon] = pd.to_numeric(hospitals[hosp_lon], errors='coerce')\n",
    "\n",
    "    acled = acled.dropna(subset=[lat_col, lon_col]).reset_index(drop=True)\n",
    "    hospitals = hospitals.dropna(subset=[hosp_lat, hosp_lon]).reset_index(drop=True)\n",
    "\n",
    "    hosp_lats = hospitals[hosp_lat].to_numpy()\n",
    "    hosp_lons = hospitals[hosp_lon].to_numpy()\n",
    "    hosp_name_col = find_col(hospitals, ['name', 'Name', 'Hospital Name', 'hospital'])\n",
    "\n",
    "    map_sub_to_cat = map_sub_to_category_map(attack_categories)\n",
    "    default_category = 'Other'\n",
    "\n",
    "    # container to collect all period binned pivots\n",
    "    combined_rows = []\n",
    "\n",
    "    for i, (start_dt, end_dt) in enumerate(date_ranges, start=1):\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"Processing date range #{i}: {start_dt.date()} to {end_dt.date()}\")\n",
    "        mask = (acled[date_col] >= pd.Timestamp(start_dt)) & (acled[date_col] <= pd.Timestamp(end_dt))\n",
    "        acled_period = acled.loc[mask].copy().reset_index(drop=True)\n",
    "        print(f\"Found {len(acled_period)} events in this period.\")\n",
    "\n",
    "        if len(acled_period) == 0:\n",
    "            # still build empty pivot with bins and zeros\n",
    "            event_present = False\n",
    "        else:\n",
    "            event_present = True\n",
    "            # compute nearest hospital for each event\n",
    "            ev_lats = acled_period[lat_col].to_numpy()[:, None]   # shape (n,1)\n",
    "            ev_lons = acled_period[lon_col].to_numpy()[:, None]   # shape (n,1)\n",
    "            # hosp arrays shapes (m,) -> broadcast to (n,m) in haversine\n",
    "            dists_km = haversine_np(ev_lons, ev_lats, hosp_lons[None, :], hosp_lats[None, :])\n",
    "            nearest_idx = np.argmin(dists_km, axis=1)\n",
    "            nearest_dist = dists_km[np.arange(dists_km.shape[0]), nearest_idx]\n",
    "\n",
    "            acled_period['nearest_hospital_index'] = nearest_idx\n",
    "            if hosp_name_col:\n",
    "                # use iloc because nearest_idx are integer positions\n",
    "                acled_period['nearest_hospital_name'] = hospitals.iloc[nearest_idx][hosp_name_col].values\n",
    "            else:\n",
    "                acled_period['nearest_hospital_name'] = hospitals.index[nearest_idx].astype(str)\n",
    "            acled_period['nearest_hospital_distance_km'] = nearest_dist\n",
    "\n",
    "            def map_sub(sub):\n",
    "                if pd.isna(sub):\n",
    "                    return default_category\n",
    "                return map_sub_to_cat.get(str(sub).strip().lower(), default_category)\n",
    "            acled_period['attack_category'] = acled_period[sub_event_col].apply(map_sub)\n",
    "\n",
    "        # Build bins: edges and human-friendly labels\n",
    "        bin_edges = build_bin_edges(start_dt, end_dt, BIN_LENGTH)\n",
    "        bin_labels = []\n",
    "        bin_starts = []\n",
    "        bin_ends = []\n",
    "        for b_start, b_end in zip(bin_edges[:-1], bin_edges[1:]):\n",
    "            # label shows inclusive start to inclusive end (we subtracted 1 day from b_end)\n",
    "            label = f\"{b_start.date()} to {(b_end - pd.Timedelta(days=1)).date()}\"\n",
    "            bin_labels.append(label)\n",
    "            bin_starts.append(b_start.date())\n",
    "            bin_ends.append((b_end - pd.Timedelta(days=1)).date())\n",
    "\n",
    "        # Create a small dataframe with bin metadata to merge later so each row gets correct start/end\n",
    "        bins_df = pd.DataFrame({\n",
    "            'period': bin_labels,\n",
    "            'bin_start': [d.isoformat() for d in bin_starts],\n",
    "            'bin_end': [d.isoformat() for d in bin_ends]\n",
    "        })\n",
    "\n",
    "        if event_present:\n",
    "            # cut into bins (right=False so bins are [start, end) and our final edge included via +1day)\n",
    "            acled_period['time_bin'] = pd.cut(acled_period[date_col],\n",
    "                                              bins=bin_edges,\n",
    "                                              labels=bin_labels,\n",
    "                                              right=False)\n",
    "            counts = (acled_period\n",
    "                      .groupby(['time_bin', 'attack_category'])\n",
    "                      .size()\n",
    "                      .reset_index(name='count'))\n",
    "            pivot = counts.pivot(index='time_bin', columns='attack_category', values='count').fillna(0)\n",
    "        else:\n",
    "            # create empty pivot with bins as index and categories as columns\n",
    "            pivot = pd.DataFrame(0, index=pd.Index(bin_labels, name='time_bin'),\n",
    "                                 columns=list(attack_categories.keys()))\n",
    "\n",
    "        # make sure every category column exists\n",
    "        pivot = ensure_all_categories(pivot, list(attack_categories.keys()))\n",
    "\n",
    "        # Reset index and rename the bin column to 'period' so it matches the screenshot\n",
    "        pivot = pivot.reset_index().rename(columns={'time_bin': 'period'})\n",
    "\n",
    "        # Merge bin-level starts/ends so each row's period_start/period_end match the 'period' text\n",
    "        pivot = pd.merge(bins_df, pivot, on='period', how='right', sort=False)\n",
    "        # If merge produced NaNs for bin_start/bin_end (shouldn't), fill with overall range start/end\n",
    "        pivot['bin_start'] = pivot['bin_start'].fillna(pd.Timestamp(start_dt).date().isoformat())\n",
    "        pivot['bin_end']   = pivot['bin_end'].fillna(pd.Timestamp(end_dt).date().isoformat())\n",
    "\n",
    "        # Add period metadata column at front: period_id (same for all rows belonging to this date-range)\n",
    "        pivot.insert(0, 'period_id', i)\n",
    "\n",
    "        # rename bin_start/bin_end to desired column names\n",
    "        pivot = pivot.rename(columns={'bin_start': 'period_start', 'bin_end': 'period_end'})\n",
    "\n",
    "        # Ensure category columns are ints and in a stable order (use attack_categories order or override)\n",
    "        desired_category_order = list(attack_categories.keys())  # <-- change this list if you want a different column order\n",
    "        for c in desired_category_order:\n",
    "            if c not in pivot.columns:\n",
    "                pivot[c] = 0\n",
    "            # fill NaNs then cast to int\n",
    "            pivot[c] = pivot[c].fillna(0).astype(int)\n",
    "\n",
    "        # Reorder columns so the table matches the screenshot and has period_start/period_end matching the period\n",
    "        ordered_cols = ['period_id', 'period_start', 'period_end', 'period'] + desired_category_order\n",
    "        # add any unexpected extra cols at the end (defensive)\n",
    "        extra_cols = [c for c in pivot.columns if c not in ordered_cols]\n",
    "        pivot = pivot[[*ordered_cols, *extra_cols]]\n",
    "\n",
    "        # store for combined table\n",
    "        combined_rows.append(pivot)\n",
    "\n",
    "\n",
    "    # after processing all periods, concatenate combined_rows into a single DataFrame\n",
    "    if len(combined_rows) > 0:\n",
    "        combined_df = pd.concat(combined_rows, ignore_index=True, sort=False)\n",
    "\n",
    "        # final column order for combined DF\n",
    "        cols = ['period_id', 'period_start', 'period_end', 'period'] + list(attack_categories.keys())\n",
    "        # add any missing columns (defensive)\n",
    "        for c in cols:\n",
    "            if c not in combined_df.columns:\n",
    "                combined_df[c] = 0\n",
    "        # ensure categories are ints\n",
    "        for c in attack_categories.keys():\n",
    "            combined_df[c] = combined_df[c].fillna(0).astype(int)\n",
    "\n",
    "        combined_df = combined_df[cols]\n",
    "    else:\n",
    "        # empty combined table with expected columns\n",
    "        combined_df = pd.DataFrame(columns=['period_id', 'period_start', 'period_end', 'period'] + list(attack_categories.keys()))\n",
    "\n",
    "    combined_path = os.path.join(output_folder, \"injuries.xlsx\")\n",
    "    with pd.ExcelWriter(combined_path, engine='openpyxl') as writer:\n",
    "        combined_df.to_excel(writer, sheet_name='all_binned_counts', index=False)\n",
    "\n",
    "    print(f\"\\nSaved combined binned counts to: {combined_path}\")\n",
    "    print(\"Done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
